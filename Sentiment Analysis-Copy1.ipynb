{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source: https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set Information:\n",
    "\n",
    "This dataset was created for the Paper 'From Group to Individual Labels using Deep Features', Kotzias et. al,. KDD 2015. It contains sentences labelled with positive or negative sentiment. \n",
    "\n",
    "Details:      \n",
    "Sentence score is either 1 (for positive) or 0 (for negative) \n",
    "The sentences come from three different websites/fields: \n",
    "- imdb.com \n",
    "- amazon.com \n",
    "- yelp.com \n",
    "\n",
    "For each website, there exist 500 positive and 500 negative sentences. Those were selected randomly for larger datasets of reviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "- [Balanced Labels](#labels)\n",
    "- [Preprocessing Sentences](#pre)\n",
    "- [Split Training and Testing Set](#split)\n",
    "- [Postprocessing](#post)\n",
    "- [Logistic Regression vs Naive Bayes](#models)\n",
    "- [N-gram Model](#2)\n",
    "- [PCA for Bag of Words](#pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/melissajin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/melissajin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/melissajin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748 1000 1000\n"
     ]
    }
   ],
   "source": [
    "#load data sets\n",
    "imdb = pd.read_csv(\"imdb_labelled.txt\", sep=\"\\t\",header =None)\n",
    "amazon = pd.read_csv(\"amazon_cells_labelled.txt\", sep=\"\\t\",header =None)\n",
    "yelp = pd.read_csv(\"yelp_labelled.txt\", sep=\"\\t\",header =None)\n",
    "print(len(imdb),len(amazon),len(yelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  A very, very, very slow-moving, aimless movie ...  0\n",
       "1  Not sure who was more lost - the flat characte...  0\n",
       "2  Attempting artiness with black & white and cle...  0\n",
       "3       Very little music or anything to speak of.    0\n",
       "4  The best scene in the movie was when Gerardo i...  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id =\"labels\">Balanced Labels</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio (positive/negative) is: 1.0662983425414365\n",
      "The ratio (positive/negative) is: 1.0\n",
      "The ratio (positive/negative) is: 1.0\n"
     ]
    }
   ],
   "source": [
    "#The function count the number of positive labels and the number of negative labels and calculate the ratio\n",
    "def labelcount(dataset):    \n",
    "    positive = 0\n",
    "    for i in range(0,len(dataset)):\n",
    "        if dataset[1][i] == 1:\n",
    "            positive +=1\n",
    "    negative = len(dataset) - positive\n",
    "    ratio = positive/negative\n",
    "    print(\"The ratio (positive/negative) is: \" + str(ratio))\n",
    "labelcount(imdb)\n",
    "labelcount(amazon)\n",
    "labelcount(yelp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id = \"pre\">Preprocessing Sentences</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert lowercase all data \n",
    "imdb = imdb.apply(lambda x: x.astype(str).str.lower())\n",
    "amazon = amazon.apply(lambda x: x.astype(str).str.lower())\n",
    "yelp = yelp.apply(lambda x: x.astype(str).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization of all words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(0,len(imdb)):\n",
    "    imdb[0][i] = nltk.word_tokenize(imdb[0][i])\n",
    "    imdb[0][i] = [lemmatizer.lemmatize(w) for w in imdb[0][i]]\n",
    "for i in range(0,len(amazon)):\n",
    "    amazon[0][i] = nltk.word_tokenize(amazon[0][i])\n",
    "    amazon[0][i] = [lemmatizer.lemmatize(w) for w in amazon[0][i]]\n",
    "for i in range(0,len(yelp)):\n",
    "    yelp[0][i] = nltk.word_tokenize(yelp[0][i])\n",
    "    yelp[0][i] = [lemmatizer.lemmatize(w) for w in yelp[0][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in range(0,len(imdb)):\n",
    "    imdb[0][i] = [word for word in imdb[0][i] if not word in stop_words]   \n",
    "for i in range(0,len(amazon)):\n",
    "    amazon[0][i] = [word for word in amazon[0][i] if not word in stop_words] \n",
    "for i in range(0,len(yelp)):\n",
    "    yelp[0][i] = [word for word in yelp[0][i] if not word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip punctuation\n",
    "string.punctuation\n",
    "exclude = set(string.punctuation)\n",
    "for i in range(0,len(imdb)):\n",
    "    imdb[0][i] = [''.join(p for p in word if p not in string.punctuation) for word in imdb[0][i]]  \n",
    "    imdb[0][i] = [word for word in imdb[0][i] if word]\n",
    "for i in range(0,len(amazon)):\n",
    "    amazon[0][i] = [''.join(p for p in word if p not in string.punctuation) for word in amazon[0][i]]\n",
    "    amazon[0][i] = [word for word in amazon[0][i] if word]\n",
    "for i in range(0,len(yelp)):\n",
    "    yelp[0][i] = [''.join(p for p in word if p not in string.punctuation) for word in yelp[0][i]]\n",
    "    yelp[0][i] = [word for word in yelp[0][i] if word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"split\">Split Training and Testing Set<a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_test,y_test=[],[],[],[]\n",
    "x_train.extend(imdb[0][:400])\n",
    "x_train.extend(amazon[0][:400])\n",
    "x_train.extend(yelp[0][:400]) \n",
    "y_train.extend(imdb[1][:400])\n",
    "y_train.extend(amazon[1][:400])\n",
    "y_train.extend(yelp[1][:400]) \n",
    "x_test.extend(imdb[0][400:500])\n",
    "x_test.extend(amazon[0][400:500])\n",
    "x_test.extend(yelp[0][400:500]) \n",
    "y_test.extend(imdb[1][400:500])\n",
    "y_test.extend(amazon[1][400:500])\n",
    "y_test.extend(yelp[1][400:500]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 1200 300 300\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train),len(train_y),len(test_x),len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [\",\".join(ele) for ele in x_train]\n",
    "x_test = [\",\".join(ele) for ele in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctV = CountVectorizer()\n",
    "#build a dictionary of unique words for training set\n",
    "x_train_bag = ctV.fit_transform(x_train).todense()\n",
    "\n",
    "ctV_test = CountVectorizer(vocabulary=ctV.get_feature_names())\n",
    "x_test_bag = ctV_test.fit_transform(x_test).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 2986) (300, 2986)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_bag.shape,x_test_bag.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"post\">Postprocessing</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data by using L2 norm for training set: x^ = x / ||x||\n",
    "#Due to the huge variance in the dataset, we want to minimize the effect, thus using L2 minimize the variance effect\n",
    "#L2 normalization is the best choice\n",
    "\n",
    "x_train_bag_norm = normalize(x_train_bag)\n",
    "\n",
    "#normalize the data by using L2 norm for testing set \n",
    "x_test_bag_norm = normalize(x_test_bag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"models\">Logistic Regression vs Naive Bayes</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression normalized by L2 norm accuracy:0.74\n"
     ]
    }
   ],
   "source": [
    "lrl2 = LogisticRegression()\n",
    "lrl2.fit(x_train_bag_norm, y_train)\n",
    "print(\"Logistic Regression normalized by L2 norm accuracy:{:.2f}\".format(lrl2.score(x_test_bag_norm ,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200 1200 300 300\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train_bag_norm),len(y_train),len(x_test_bag_norm),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:0.69\n",
      "Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:0.72\n"
     ]
    }
   ],
   "source": [
    "gaussian = GaussianNB()\n",
    "gaussian.fit(x_train_bag_norm, y_train)\n",
    "print(\"Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:{:.2f}\"\n",
    "      .format(gaussian.score(x_test_bag_norm ,y_test)))\n",
    "\n",
    "bernoulli = BernoulliNB()\n",
    "bernoulli.fit(x_train_bag_norm, y_train)\n",
    "print(\"Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:{:.2f}\"\n",
    "      .format(bernoulli.score(x_test_bag_norm ,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression model performs slightly better than Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with 10 highest values:\n",
      "('zombiez', 2985)\n",
      "('zombiestudents', 2984)\n",
      "('zillion', 2983)\n",
      "('zero', 2982)\n",
      "('yun', 2981)\n",
      "('yummy', 2980)\n",
      "('yum', 2979)\n",
      "('yukon', 2978)\n",
      "('yucky', 2977)\n",
      "('youthful', 2976)\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab = sorted(ctV.vocabulary_.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Dictionary with 10 highest values:\")   \n",
    "for word in sorted_vocab[:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id =\"2\">N-gram Model</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctV2 = CountVectorizer(ngram_range=(2, 2))\n",
    "#build a 2-gram dictionary of unique words for training set\n",
    "x_train_bag2 = ctV2.fit_transform(x_train).todense()\n",
    "\n",
    "ctV_test2 = CountVectorizer(vocabulary=ctV2.get_feature_names())\n",
    "x_test_bag2 = ctV_test2.fit_transform(x_test).todense()\n",
    "\n",
    "#postprocessing\n",
    "x_train_bag_norm2 = normalize(x_train_bag2)\n",
    "\n",
    "#normalize the data by using L2 norm for testing set \n",
    "x_test_bag_norm2 = normalize(x_test_bag2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression normalized by L2 norm accuracy:0.41\n",
      "Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:0.59\n",
      "Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:0.59\n"
     ]
    }
   ],
   "source": [
    "lrl2_2 = LogisticRegression()\n",
    "lrl2_2.fit(x_train_bag_norm2, y_train)\n",
    "print(\"Logistic Regression normalized by L2 norm accuracy:{:.2f}\".format(lrl2_2.score(x_test_bag_norm2 ,y_test)))\n",
    "\n",
    "gaussian2 = GaussianNB()\n",
    "gaussian2.fit(x_train_bag_norm2, y_train)\n",
    "print(\"Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:{:.2f}\"\n",
    "      .format(gaussian2.score(x_test_bag_norm2 ,y_test)))\n",
    "\n",
    "bernoulli2 = BernoulliNB()\n",
    "bernoulli2.fit(x_train_bag_norm2, y_train)\n",
    "print(\"Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:{:.2f}\"\n",
    "      .format(bernoulli2.score(x_test_bag_norm2 ,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with 10 highest values:\n",
      "('zombiez part', 7181)\n",
      "('zombiestudents back', 7180)\n",
      "('zillion time', 7179)\n",
      "('zero taste', 7178)\n",
      "('zero star', 7177)\n",
      "('yun fat', 7176)\n",
      "('yummy try', 7175)\n",
      "('yum yum', 7174)\n",
      "('yum sauce', 7173)\n",
      "('yukon gold', 7172)\n"
     ]
    }
   ],
   "source": [
    "sorted_vocab2 = sorted(ctV2.vocabulary_.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print(\"Dictionary with 10 highest values:\")   \n",
    "for word in sorted_vocab2[:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"pca\">PCA for Bag of Words<a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression normalized by L2 norm accuracy:0.55\n",
      "Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:0.51\n",
      "Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:0.52\n"
     ]
    }
   ],
   "source": [
    "#PCA Features = 10\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "x_trainpca = pca.fit_transform(x_train_bag_norm) \n",
    "x_testpca = pca.transform(x_test_bag_norm) \n",
    "\n",
    "lr_pca = LogisticRegression() \n",
    "lr_pca.fit(x_trainpca, y_train)\n",
    "print(\"Logistic Regression normalized by L2 norm accuracy:{:.2f}\".format(lr_pca.score(x_testpca ,y_test)))\n",
    "\n",
    "gaussian_pca = GaussianNB()\n",
    "gaussian_pca.fit(x_trainpca, y_train)\n",
    "print(\"Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:{:.2f}\"\n",
    "      .format(gaussian_pca.score(x_testpca ,y_test)))\n",
    "\n",
    "bernoulli_pca = BernoulliNB()\n",
    "bernoulli_pca.fit(x_trainpca, y_train)\n",
    "print(\"Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:{:.2f}\"\n",
    "      .format(bernoulli_pca.score(x_testpca ,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression normalized by L2 norm accuracy:0.67\n",
      "Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:0.62\n",
      "Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:0.59\n"
     ]
    }
   ],
   "source": [
    "#PCA Features = 50\n",
    "pca = PCA(n_components=50)\n",
    "\n",
    "x_trainpca = pca.fit_transform(x_train_bag_norm) \n",
    "x_testpca = pca.transform(x_test_bag_norm) \n",
    "\n",
    "lr_pca = LogisticRegression() \n",
    "lr_pca.fit(x_trainpca, y_train)\n",
    "print(\"Logistic Regression normalized by L2 norm accuracy:{:.2f}\".format(lr_pca.score(x_testpca ,y_test)))\n",
    "\n",
    "gaussian_pca = GaussianNB()\n",
    "gaussian_pca.fit(x_trainpca, y_train)\n",
    "print(\"Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:{:.2f}\"\n",
    "      .format(gaussian_pca.score(x_testpca ,y_test)))\n",
    "\n",
    "bernoulli_pca = BernoulliNB()\n",
    "bernoulli_pca.fit(x_trainpca, y_train)\n",
    "print(\"Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:{:.2f}\"\n",
    "      .format(bernoulli_pca.score(x_testpca ,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression normalized by L2 norm accuracy:0.69\n",
      "Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:0.65\n",
      "Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:0.60\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=100)\n",
    "\n",
    "x_trainpca = pca.fit_transform(x_train_bag_norm) \n",
    "x_testpca = pca.transform(x_test_bag_norm) \n",
    "\n",
    "lr_pca = LogisticRegression() \n",
    "lr_pca.fit(x_trainpca, y_train)\n",
    "print(\"Logistic Regression normalized by L2 norm accuracy:{:.2f}\".format(lr_pca.score(x_testpca ,y_test)))\n",
    "\n",
    "gaussian_pca = GaussianNB()\n",
    "gaussian_pca.fit(x_trainpca, y_train)\n",
    "print(\"Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:{:.2f}\"\n",
    "      .format(gaussian_pca.score(x_testpca ,y_test)))\n",
    "\n",
    "bernoulli_pca = BernoulliNB()\n",
    "bernoulli_pca.fit(x_trainpca, y_train)\n",
    "print(\"Navie Bayes Classifier with Gaussian assumption normalized by L2 norm accuracy:{:.2f}\"\n",
    "      .format(bernoulli_pca.score(x_testpca ,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words using logistic regression performs the best. It reserves all features of words with every single word being captured. The 2-gram method is worse off because it introduces unnecessary features which are more sparse, and it is more biased. The reason that PCA doesn't work well because when it reduces dimension, it also losses some information.    \n",
    "Regarding to online review language usage, we notice that people always use similar keywords for each label, refer to top 20 words for positive label and negative label in part f and g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
